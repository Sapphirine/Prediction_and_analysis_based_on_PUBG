{"cells":[{"cell_type":"code","source":["dataset = spark.table(\"pugs\")\ncols = dataset.columns"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["display(dataset)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n\nstages = [] # stages in our Pipeline\nnumericCols = [\"solo_KillDeathRatio\",\"solo_TimeSurvived\",\"solo_RoundsPlayed\",\"solo_WinTop10Ratio\",\"solo_Top10s\",\"solo_Top10Ratio\",\"solo_Rating\",\"solo_BestRating\",\"solo_DamagePg\",\"solo_HeadshotKillsPg\",\"solo_KillsPg\",\"solo_RoadKillsPg\",\"solo_TeamKillsPg\",\"solo_TimeSurvivedPg\",\"solo_Top10sPg\",\"solo_Kills\",\"solo_Assists\",\"solo_HeadshotKills\",\"solo_HeadshotKillRatio\",\"solo_VehicleDestroys\",\"solo_RoadKills\",\"solo_DailyKills\",\"solo_WeeklyKills\",\"solo_RoundMostKills\",\"solo_LongestTimeSurvived\",\"solo_RideDistance\",\"solo_LongestKill\",\"solo_Heals\",\"solo_Revives\",\"solo_Boosts\",\"solo_DamageDealt\",\"solo_DBNOs\"]\nassembler = VectorAssembler(inputCols=numericCols, outputCol=\"features\")\nstages += [assembler]"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["pipeline = Pipeline(stages=stages)\n# Run the feature transformations.\n#  - fit() computes feature statistics as needed.\n#  - transform() actually transforms the features.\npipelineModel = pipeline.fit(dataset)\ndataset = pipelineModel.transform(dataset)\n\n# Keep relevant columns\nselectedcols = [\"solo_Wins\",\"features\"] + numericCols\ndataset = dataset.select(selectedcols)\ndisplay(dataset)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["display(dataset)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["(trainingData, testData) = dataset.randomSplit([0.9, 0.1], seed = 100)\nprint trainingData.count()\nprint testData.count()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n\n# Create initial LogisticRegression model\nlr = LogisticRegression(labelCol=\"solo_Wins\", featuresCol=\"features\", maxIter=10)\n\n# Train model with Training Data\nlrModel = lr.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["predictions = lrModel.transform(testData)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["predictions.printSchema()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# selected = predictions.select(\"solo_Suicides\",\"solo_Wins\",\"solo_TimeSurvived\", \"prediction\", \"solo_WinRatio\", \"solo_DamagePg\",\"rawPrediction\")\nselected = predictions.select(\"solo_Wins\", \"prediction\")\ndisplay(selected)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",labelCol=\"solo_Suicides\")\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["from pyspark.ml.classification import DecisionTreeClassifier\n\n# Create initial Decision Tree Model\ndt = DecisionTreeClassifier(labelCol=\"solo_Suicides\", featuresCol=\"features\", maxDepth=3)\n\n# Train model with Training Data\nlrModel = dt.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["#The following part is decision trees."],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["from pyspark.ml.classification import DecisionTreeClassifier\n\n# Create initial Decision Tree Model\ndt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=3)\n\n# Train model with Training Data\ndtModel = dt.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["print \"numNodes = \", dtModel.numNodes\nprint \"depth = \", dtModel.depth"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["from pyspark.ml.classification import DecisionTreeClassifier\n\n# Create initial Decision Tree Model\ndt = DecisionTreeClassifier(labelCol=\"solo_Suicides\", featuresCol=\"features\", maxDepth=3)\n\n# Train model with Training Data\nlrModel = dt.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":16}],"metadata":{"name":"project","notebookId":4081982642597361},"nbformat":4,"nbformat_minor":0}
